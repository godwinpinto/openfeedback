{
  "$schema": "https://ui.shadcn.com/schema/registry-item.json",
  "name": "use-genui-hybrid-llm",
  "type": "registry:hook",
  "title": "GenUI Hybrid LLM",
  "description": "GenUI-powered hybrid LLM component with automatic text rewriting and tone customization.",
  "dependencies": [
    "@ai-sdk/react",
    "ai"
  ],
  "files": [
    {
      "path": "registry/new-york/gencn-ui/items/hybrid-llm/use-genui-hybrid-llm.ts",
      "content": "'use client';\n\nimport { useChat, type UseChatHelpers, type UseChatOptions } from '@ai-sdk/react';\nimport { DefaultChatTransport, type UIMessage } from 'ai';\nimport { useEffect, useMemo, useRef, useState } from 'react';\nimport { LocalChatTransport, type LocalChatTransportOptions } from '@/registry/new-york/gencn-ui/items/hybrid-llm/genui-local-chat-transport';\n\nexport type TransportMode = 'local' | 'remote';\n\nexport interface UseGenUIHybridLLMOptions extends Omit<UseChatOptions<UIMessage>, 'transport' | 'id'> {\n  /**\n   * Initial transport mode. Defaults to 'remote' if local is not available.\n   */\n  initialMode?: TransportMode;\n  /**\n   * Options for local transport when using Chrome LanguageModel API\n   */\n  localTransportOptions?: LocalChatTransportOptions;\n  /**\n   * API endpoint for remote transport. Defaults to '/api/chat'\n   */\n  remoteApiEndpoint?: string;\n  /**\n   * Whether to automatically use local transport if available\n   */\n  autoUseLocalIfAvailable?: boolean;\n  /**\n   * Chat ID prefix. The actual ID will be prefixed with the transport mode.\n   */\n  chatIdPrefix?: string;\n}\n\nexport interface UseGenUIHybridLLMReturn extends UseChatHelpers<UIMessage> {\n  /**\n   * Current transport mode ('local' or 'remote')\n   */\n  transportMode: TransportMode;\n  /**\n   * Whether local LLM is supported in this browser\n   */\n  isLocalSupported: boolean;\n  /**\n   * Whether local LLM is available (may need download)\n   */\n  localAvailability: 'available' | 'downloadable' | 'unavailable' | 'checking';\n  /**\n   * Switch to a different transport mode\n   */\n  setTransportMode: (mode: TransportMode) => void;\n  /**\n   * Current transport instance\n   */\n  transport: LocalChatTransport | DefaultChatTransport<UIMessage>;\n}\n\nexport function useGenUIHybridLLM(\n  options: UseGenUIHybridLLMOptions = {}\n): UseGenUIHybridLLMReturn {\n  const {\n    initialMode,\n    localTransportOptions = {},\n    remoteApiEndpoint = '/api/chat',\n    autoUseLocalIfAvailable = false,\n    chatIdPrefix = 'chat',\n    ...useChatOptions\n  } = options;\n\n  // State for transport mode\n  const [transportMode, setTransportModeState] = useState<TransportMode>(() => {\n    // We'll check support and set mode after mount to avoid hydration mismatch\n    return initialMode || 'remote';\n  });\n\n  // State for local LLM support and availability\n  const [isLocalSupported, setIsLocalSupported] = useState(false);\n  const [localAvailability, setLocalAvailability] = useState<'available' | 'downloadable' | 'unavailable' | 'checking'>('checking');\n\n  // Preserve messages across transport switches\n  const preservedMessagesRef = useRef<UIMessage[]>([]);\n  const prevTransportModeRef = useRef<TransportMode>(transportMode);\n\n  // Check local LLM support and availability after mount (client-only)\n  useEffect(() => {\n    const checkLocalSupport = async () => {\n      const supported = LocalChatTransport.isSupported();\n      setIsLocalSupported(supported);\n\n      if (supported) {\n        setLocalAvailability('checking');\n        try {\n          const availability = await LocalChatTransport.checkAvailability({\n            expectedInputs: localTransportOptions.expectedInputs,\n            expectedOutputs: localTransportOptions.expectedOutputs,\n          });\n          setLocalAvailability(availability);\n          \n          // Auto-enable local if available and autoUseLocalIfAvailable is true\n          if (autoUseLocalIfAvailable && availability !== 'unavailable' && initialMode !== 'remote') {\n            setTransportModeState('local');\n          }\n        } catch {\n          setLocalAvailability('unavailable');\n        }\n      } else {\n        setLocalAvailability('unavailable');\n      }\n    };\n\n    checkLocalSupport();\n  }, [autoUseLocalIfAvailable, initialMode, localTransportOptions.expectedInputs, localTransportOptions.expectedOutputs]);\n\n  // Determine which transport to use\n  const shouldUseLocal = useMemo(() => {\n    return transportMode === 'local' && isLocalSupported && localAvailability !== 'unavailable';\n  }, [transportMode, isLocalSupported, localAvailability]);\n\n  // Create transport based on mode\n  const transport = useMemo(() => {\n    if (shouldUseLocal) {\n      return new LocalChatTransport({\n        system: 'You are a helpful assistant.',\n        temperature: 1.0,\n        ...localTransportOptions,\n      });\n    } else {\n      return new DefaultChatTransport({\n        api: remoteApiEndpoint,\n      });\n    }\n  }, [shouldUseLocal, localTransportOptions, remoteApiEndpoint]);\n\n  // Generate unique chat ID based on transport mode\n  const chatId = useMemo(() => {\n    return `${chatIdPrefix}-${transportMode}`;\n  }, [chatIdPrefix, transportMode]);\n\n  // Use the useChat hook with the appropriate transport\n  const chatHelpers = useChat({\n    ...useChatOptions,\n    transport,\n    id: chatId,\n  });\n\n  // Preserve messages before switching transports\n  useEffect(() => {\n    if (chatHelpers.messages.length > 0) {\n      preservedMessagesRef.current = chatHelpers.messages;\n    }\n  }, [chatHelpers.messages]);\n\n  // Restore messages when switching transports\n  useEffect(() => {\n    // Only restore if we just switched transports and messages were cleared\n    if (\n      prevTransportModeRef.current !== transportMode &&\n      preservedMessagesRef.current.length > 0 &&\n      chatHelpers.messages.length === 0 &&\n      chatHelpers.status === 'ready'\n    ) {\n      chatHelpers.setMessages(preservedMessagesRef.current);\n    }\n    prevTransportModeRef.current = transportMode;\n  }, [transportMode, chatHelpers.messages.length, chatHelpers.status, chatHelpers.setMessages]);\n\n  // Function to switch transport mode\n  const setTransportMode = (mode: TransportMode) => {\n    // Only allow switching if not currently streaming\n    if (chatHelpers.status !== 'submitted' && chatHelpers.status !== 'streaming') {\n      // Preserve current messages before switching\n      if (chatHelpers.messages.length > 0) {\n        preservedMessagesRef.current = chatHelpers.messages;\n      }\n      setTransportModeState(mode);\n    }\n  };\n\n  return {\n    ...chatHelpers,\n    transportMode,\n    isLocalSupported,\n    localAvailability,\n    setTransportMode,\n    transport,\n  };\n}\n\n",
      "type": "registry:hook"
    },
    {
      "path": "registry/new-york/gencn-ui/items/hybrid-llm/genui-local-chat-transport.ts",
      "content": "'use client';\n\nimport type { ChatTransport, UIMessage, UIMessageChunk } from 'ai';\n\n// Type definitions for Chrome LanguageModel API\ntype LanguageModelAvailability = 'available' | 'downloadable' | 'unavailable';\n\ninterface LanguageModelParams {\n  defaultTopK: number;\n  maxTopK: number;\n  defaultTemperature: number;\n  maxTemperature: number;\n}\n\ninterface LanguageModelCreateOptions {\n  topK?: number;\n  temperature?: number;\n  initialPrompts?: Array<{\n    role: 'system' | 'user' | 'assistant';\n    content: string | Array<{ type: 'text' | 'image' | 'audio'; value: string | File | HTMLCanvasElement }>;\n    prefix?: boolean;\n  }>;\n  expectedInputs?: Array<{ type: 'text' | 'image' | 'audio'; languages?: string[] }>;\n  expectedOutputs?: Array<{ type: 'text'; languages?: string[] }>;\n  signal?: AbortSignal;\n  monitor?: (monitor: any) => void;\n}\n\ninterface LanguageModelSession {\n  prompt(messages: Array<{ role: 'user' | 'assistant'; content: string | Array<{ type: 'text' | 'image' | 'audio'; value: string | File | HTMLCanvasElement }>; prefix?: boolean }>, options?: { signal?: AbortSignal; responseConstraint?: any }): Promise<string>;\n  promptStreaming(messages: Array<{ role: 'user' | 'assistant'; content: string | Array<{ type: 'text' | 'image' | 'audio'; value: string | File | HTMLCanvasElement }>; prefix?: boolean }>, options?: { signal?: AbortSignal; responseConstraint?: any }): ReadableStream<string>;\n  append(messages: Array<{ role: 'user' | 'assistant'; content: string | Array<{ type: 'text' | 'image' | 'audio'; value: string | File | HTMLCanvasElement }>; prefix?: boolean }>): Promise<void>;\n  clone(options?: { signal?: AbortSignal }): Promise<LanguageModelSession>;\n  destroy(): void;\n  inputUsage: number;\n  inputQuota: number;\n}\n\ninterface LanguageModelGlobal {\n  availability(options?: { expectedInputs?: Array<{ type: 'text' | 'image' | 'audio'; languages?: string[] }>; expectedOutputs?: Array<{ type: 'text'; languages?: string[] }> }): Promise<LanguageModelAvailability>;\n  params(): Promise<LanguageModelParams>;\n  create(options?: LanguageModelCreateOptions): Promise<LanguageModelSession>;\n}\n\n\nexport interface LocalChatTransportOptions {\n  system?: string;\n  temperature?: number;\n  topK?: number;\n  expectedInputs?: Array<{ type: 'text' | 'image' | 'audio'; languages?: string[] }>;\n  expectedOutputs?: Array<{ type: 'text'; languages?: string[] }>;\n  onProgress?: (percent: number) => void;\n}\n\n/**\n * LocalChatTransport uses Chrome's LanguageModel API to run chat locally\n * instead of making HTTP requests to a server.\n */\nexport class LocalChatTransport implements ChatTransport<UIMessage> {\n  private session: LanguageModelSession | null = null;\n  private sessionAbortController: AbortController | null = null;\n  private options: LocalChatTransportOptions;\n  private isInitialized = false;\n  private processedMessageCount = 0; // Track how many messages we've processed in the session\n\n  constructor(options: LocalChatTransportOptions = {}) {\n    this.options = options;\n  }\n\n  /**\n   * Check if LanguageModel API is supported in the current environment\n   */\n  static isSupported(): boolean {\n    if (typeof self === 'undefined') return false;\n    return 'LanguageModel' in (self as any);\n  }\n\n  /**\n   * Check LanguageModel availability\n   */\n  static async checkAvailability(\n    options?: { expectedInputs?: Array<{ type: 'text' | 'image' | 'audio'; languages?: string[] }>; expectedOutputs?: Array<{ type: 'text'; languages?: string[] }> }\n  ): Promise<LanguageModelAvailability> {\n    if (!this.isSupported()) return 'unavailable';\n    try {\n      const globalSelf = (typeof self !== 'undefined' ? self : window) as any;\n      const LanguageModel = globalSelf.LanguageModel as LanguageModelGlobal;\n      return await LanguageModel.availability(options);\n    } catch {\n      return 'unavailable';\n    }\n  }\n\n  /**\n   * Initialize or reuse the LanguageModel session\n   */\n  private async ensureSession(): Promise<LanguageModelSession> {\n    if (this.session && this.isInitialized) {\n      return this.session;\n    }\n\n    const globalSelf = (typeof self !== 'undefined' ? self : window) as any;\n    if (typeof globalSelf.LanguageModel === 'undefined') {\n      throw new Error('Chrome LanguageModel API is not available.');\n    }\n\n    const LanguageModel = globalSelf.LanguageModel as LanguageModelGlobal;\n\n    // Check availability\n    const availability = await LanguageModel.availability({\n      expectedInputs: this.options.expectedInputs,\n      expectedOutputs: this.options.expectedOutputs,\n    });\n\n    if (availability === 'unavailable') {\n      throw new Error('LanguageModel is not available on this device.');\n    }\n\n    // Get params for defaults\n    const params = await LanguageModel.params();\n\n    // Create abort controller for session\n    this.sessionAbortController = new AbortController();\n\n    // Prepare initial prompts (system message only at creation time)\n    const initialPrompts: Array<{ role: 'system'; content: string }> = [];\n    if (this.options.system) {\n      initialPrompts.push({\n        role: 'system',\n        content: this.options.system,\n      });\n    }\n\n    // Create session with monitor for download progress\n    const createOptions: LanguageModelCreateOptions = {\n      temperature: this.options.temperature ?? params.defaultTemperature,\n      topK: this.options.topK ?? params.defaultTopK,\n      expectedInputs: this.options.expectedInputs,\n      expectedOutputs: this.options.expectedOutputs,\n      signal: this.sessionAbortController.signal,\n      initialPrompts: initialPrompts.length > 0 ? initialPrompts : undefined,\n      monitor: (m: any) => {\n        try {\n          m?.addEventListener?.('downloadprogress', (e: any) => {\n            if (typeof e.loaded === 'number' && typeof e.total === 'number') {\n              const percent = Math.round((e.loaded / e.total) * 100);\n              this.options.onProgress?.(percent);\n            }\n          });\n        } catch {}\n      },\n    };\n\n    this.session = await LanguageModel.create(createOptions);\n    this.isInitialized = true;\n\n    return this.session;\n  }\n\n  /**\n   * Convert UIMessage to LanguageModel message format for appending\n   */\n  private convertMessageForAppend(message: UIMessage): Array<{\n    role: 'user' | 'assistant';\n    content: string;\n  }> {\n    if (!message.parts || !Array.isArray(message.parts)) {\n      return [];\n    }\n\n    const textParts = message.parts\n      .filter((part) => {\n        if (!part) {\n          return false;\n        }\n        return part.type === 'text';\n      })\n      .map((part) => {\n        if (!part) {\n          return '';\n        }\n        const text = (part as any)?.text;\n        if (text === undefined) {\n          return '';\n        }\n        return text;\n      })\n      .filter(text => text !== undefined && text !== null)\n      .join('');\n\n    if (!textParts.trim()) {\n      return [];\n    }\n\n    return [\n      {\n        role: message.role === 'user' ? 'user' : 'assistant',\n        content: textParts,\n      },\n    ];\n  }\n\n  /**\n   * Convert UIMessage to LanguageModel message format\n   */\n  private convertMessageToLanguageModelFormat(message: UIMessage): Array<{\n    role: 'user' | 'assistant';\n    content: string | Array<{ type: 'text'; value: string }>;\n  }> {\n    if (!message.parts || !Array.isArray(message.parts)) {\n      return [];\n    }\n\n    const textParts = message.parts\n      .filter((part) => {\n        if (!part) {\n          return false;\n        }\n        return part.type === 'text';\n      })\n      .map((part) => {\n        if (!part) {\n          return '';\n        }\n        const text = (part as any)?.text;\n        if (text === undefined) {\n          return '';\n        }\n        return text;\n      })\n      .filter(text => text !== undefined && text !== null)\n      .join('');\n\n    if (!textParts.trim()) {\n      return [];\n    }\n\n    return [\n      {\n        role: message.role === 'user' ? 'user' : 'assistant',\n        content: textParts,\n      },\n    ];\n  }\n\n  /**\n   * Stream chat response using LanguageModel API\n   */\n  async *streamChat(messages: UIMessage[], options?: { signal?: AbortSignal }): AsyncGenerator<UIMessageChunk> {\n    if (messages.length === 0) {\n      throw new Error('No messages provided');\n    }\n\n    const lastMessage = messages[messages.length - 1];\n    \n    if (!lastMessage) {\n      throw new Error('Last message is undefined');\n    }\n    \n    if (lastMessage.role !== 'user') {\n      throw new Error('Last message must be from user');\n    }\n\n    try {\n      // Ensure session is initialized\n      const session = await this.ensureSession();\n\n      // Append any new messages to the session context (excluding the last user message)\n      // The session maintains context, so we only need to append messages that haven't been processed yet\n      const messagesToAppend = messages.slice(this.processedMessageCount, -1);\n      \n      for (const message of messagesToAppend) {\n        // Skip if this is the assistant response we just generated (it will be in the messages array)\n        // The assistant response should not be appended as it's the result of our prompt\n        if (message.role === 'assistant') {\n          continue;\n        }\n        \n        const languageModelMessages = this.convertMessageForAppend(message);\n        if (languageModelMessages.length > 0) {\n          await session.append(languageModelMessages);\n          this.processedMessageCount++;\n        }\n      }\n\n      // Convert the last user message for prompting\n      const languageModelMessages = this.convertMessageToLanguageModelFormat(lastMessage);\n\n      if (languageModelMessages.length === 0) {\n        throw new Error('Message has no text content');\n      }\n\n      // Use promptStreaming for streaming responses\n      const stream = session.promptStreaming(languageModelMessages, {\n        signal: options?.signal,\n      });\n\n      let fullText = '';\n      let messageId: string | undefined;\n\n      // Generate a message ID\n      messageId = `msg-${Date.now()}-${Math.random().toString(36).slice(2)}`;\n\n      // Generate a text part ID (separate from messageId, used for text-start/delta/end chunks)\n      // Following the pattern from transformTextToUiMessageStream\n      const textPartId = `text-1`;\n\n      // Yield text chunks as they come\n      // ReadableStream<string> needs to be consumed with reader\n      const reader = stream.getReader();\n      \n      // Yield text-start to initialize the text part\n      // This matches the pattern from transformTextToUiMessageStream\n      yield {\n        type: 'text-start',\n        id: textPartId,\n      } as UIMessageChunk;\n      \n      try {\n        while (true) {\n          const readResult = await reader.read();\n          \n          const { done, value } = readResult;\n          if (done) {\n            // Yield text-end to finalize the text part before finish\n            yield {\n              type: 'text-end',\n              id: textPartId,\n            } as UIMessageChunk;\n            // Yield finish chunk immediately when stream is done\n            yield {\n              type: 'finish',\n              id: messageId,\n            } as UIMessageChunk;\n            break;\n          }\n          \n          if (typeof value === 'string') {\n            fullText += value;\n            \n            // Yield text-delta chunk with the text part ID\n            yield {\n              type: 'text-delta',\n              id: textPartId,\n              delta: value,\n            } as UIMessageChunk;\n          }\n        }\n      } catch (streamError) {\n        // If error occurs, try to yield text-end before throwing\n        try {\n          yield {\n            type: 'text-end',\n            id: textPartId,\n          } as UIMessageChunk;\n        } catch {\n          // Ignore if we can't yield text-end\n        }\n        throw streamError;\n      } finally {\n        reader.releaseLock();\n      }\n\n      // Update processed message count after successful response\n      // Note: We add 1 to account for the assistant response that will be added\n      this.processedMessageCount = messages.length + 1;\n    } catch (error) {\n      // Yield error chunk\n      yield {\n        type: 'error',\n        errorText: error instanceof Error ? error.message : String(error),\n      } as UIMessageChunk;\n      throw error;\n    }\n  }\n\n  /**\n   * Send messages and get streaming response\n   * This method is called by useChat hook\n   */\n  async sendMessages(options: {\n    trigger: 'submit-message' | 'regenerate-message';\n    chatId: string;\n    messageId: string | undefined;\n    messages: UIMessage[];\n    abortSignal: AbortSignal | undefined;\n    [key: string]: unknown;\n  }): Promise<ReadableStream<UIMessageChunk>> {\n    const self = this;\n    const { messages, abortSignal } = options;\n    \n    if (!messages || !Array.isArray(messages)) {\n      throw new Error('messages must be an array');\n    }\n\n    // Create a readable stream\n    const stream = new ReadableStream<UIMessageChunk>({\n      async start(controller) {\n        let streamClosed = false; // Declare before abort handler\n        let chunkIndex = 0;\n        \n        try {\n          // Handle abort signal\n          if (abortSignal) {\n            abortSignal.addEventListener('abort', () => {\n              streamClosed = true;\n              if (self.sessionAbortController) {\n                self.sessionAbortController.abort();\n              }\n              try {\n                controller.close();\n              } catch {\n                // May already be closed\n              }\n            });\n          }\n\n          // Stream chunks\n          let finishChunkReceived = false;\n          let finishChunkEnqueued = false;\n          \n          try {\n            for await (const chunk of self.streamChat(messages, { signal: abortSignal })) {\n              chunkIndex++;\n              \n              // Track if this is the finish chunk\n              if (chunk.type === 'finish') {\n                finishChunkReceived = true;\n              }\n              \n              // If stream was already closed by consumer, skip non-finish chunks\n              // But we still want to process the finish chunk even if stream is closed\n              if (streamClosed && chunk.type !== 'finish') {\n                // Skip non-finish chunks if stream is already closed\n                continue;\n              }\n              \n              try {\n                // Try to enqueue the chunk (if stream is closed, this will fail for finish chunk too)\n                controller.enqueue(chunk);\n                \n                // Track if finish chunk was successfully enqueued\n                if (chunk.type === 'finish') {\n                  finishChunkEnqueued = true;\n                  // Once we've processed the finish chunk, we can stop\n                  break;\n                }\n              } catch (enqueueError: any) {\n                // If stream was closed by consumer, that's okay\n                if (enqueueError?.message?.includes('closed') || enqueueError?.message?.includes('Cannot enqueue')) {\n                  // Mark stream as closed if not already\n                  if (!streamClosed) {\n                    streamClosed = true;\n                  }\n                  \n                  // If this was a finish chunk, we've received it even if we can't enqueue it\n                  if (chunk.type === 'finish') {\n                    finishChunkReceived = true;\n                    // We've received the finish chunk, so we can stop now\n                    break;\n                  } else {\n                    // For non-finish chunks, continue waiting for the finish chunk\n                    // Don't break - continue the loop\n                  }\n                } else {\n                  throw enqueueError;\n                }\n              }\n            }\n          } catch (streamError: any) {\n            // If the async generator throws because stream was closed, that's okay\n            if (streamError?.message?.includes('closed') || streamError?.message?.includes('aborted')) {\n              streamClosed = true;\n            } else {\n              throw streamError;\n            }\n          }\n          \n          // Close the controller if stream wasn't already closed\n          if (!streamClosed) {\n            try {\n              controller.close();\n            } catch {\n              // May already be closed\n            }\n          }\n        } catch (error) {\n          controller.error(error);\n        }\n      },\n    });\n\n    return stream;\n  }\n\n  /**\n   * Reconnect to an existing stream (not applicable for local transport, but required by interface)\n   */\n  async reconnectToStream(_options: {\n    chatId: string;\n    [key: string]: unknown;\n  }): Promise<ReadableStream<UIMessageChunk> | null> {\n    // Local transport doesn't support reconnection\n    // Return null to indicate reconnection is not supported\n    return null;\n  }\n\n  /**\n   * Cleanup: destroy the session\n   */\n  destroy(): void {\n    if (this.session) {\n      try {\n        this.session.destroy();\n      } catch {}\n      this.session = null;\n    }\n    if (this.sessionAbortController) {\n      this.sessionAbortController.abort();\n      this.sessionAbortController = null;\n    }\n    this.isInitialized = false;\n    this.processedMessageCount = 0;\n  }\n}\n\n",
      "type": "registry:lib"
    }
  ]
}